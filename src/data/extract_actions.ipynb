{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(s):\n",
    "    name = ''\n",
    "    sentence = ''\n",
    "    flag = 0\n",
    "    for w in s:\n",
    "        if w != ':' and flag == 0:\n",
    "            name += w\n",
    "        if w == ':':\n",
    "            flag = 1\n",
    "        elif flag == 1:\n",
    "            sentence += w\n",
    "    return name, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversation(data):\n",
    "    conversations = []\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        if len(data[i]['dialogue'].split('\\r\\n')) > 1:\n",
    "            sentences = data[i]['dialogue'].replace(\"|\", \" \").split('\\r\\n')\n",
    "            \n",
    "        else:\n",
    "            sentences = data[i]['dialogue'].replace(\"|\", \" \").split('\\n')\n",
    "            \n",
    "        if len(sentences) == 1:\n",
    "            continue\n",
    "            \n",
    "        conversations.append(sentences)\n",
    "    \n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "import allennlp_models.structured_prediction\n",
    "import os\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "# Add neural coref to SpaCy's pipe\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "def coreference(s):\n",
    "    doc = nlp(s)\n",
    "    return doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_coref(full_conv, index, word):\n",
    "    ref_list = ['he', 'she', 'him', 'her', 'it', 'his']\n",
    "    prev_sentence = ''\n",
    "    for i in range(index, -1, -1):\n",
    "        #print(i)\n",
    "        cur_sentence =  full_conv[i][1]\n",
    "        prev_sentence = cur_sentence + ' ' + prev_sentence\n",
    "        coref = coreference(prev_sentence)\n",
    "        #print('----')\n",
    "        #print(prev_sentence)\n",
    "        #print(coref)\n",
    "        #print('----')\n",
    "        if len(coref) > 0:\n",
    "            for clusters in coref:\n",
    "                if word in [str(w) for w in clusters.mentions] and str(clusters.main).lower() not in ref_list:\n",
    "                    return str(clusters.main)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ['i', 'me', 'my']\n",
    "second = ['you', 'u', 'your']\n",
    "third = ['he', 'she', 'him', 'her', 'it', 'his']\n",
    "\n",
    "def transform_perspective(conv):\n",
    "    transformed_conv = ''\n",
    "    temp = []\n",
    "    flag = 0\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(conv)):\n",
    "        #print(i)\n",
    "        cur_name = conv[i][0]\n",
    "        cur_uttr = conv[i][1]\n",
    "        s = ''\n",
    "        \n",
    "\n",
    "        for word in word_tokenize(cur_uttr):\n",
    "            if word.lower() in first:\n",
    "                temp_word = cur_name\n",
    "            elif word.lower() in second:\n",
    "                try:\n",
    "                    temp_word = conv[i+1][0]\n",
    "                except:\n",
    "                    temp_word = conv[i-1][0]\n",
    "            elif word.lower() in third:\n",
    "                temp_word = find_nearest_coref(conv, i, word)\n",
    "            else:\n",
    "                temp_word = word\n",
    "\n",
    "            s = s + temp_word + ' '\n",
    "\n",
    "        #j = len(s)-1\n",
    "        #while s[j] == ' ':\n",
    "        #    j = j - 1\n",
    "\n",
    "        #if s[j] not in punc:\n",
    "        s += '.'\n",
    "\n",
    "\n",
    "        temp.append(s)\n",
    "                \n",
    "                \n",
    "        \n",
    "    transformed_conv = ' '.join(temp)\n",
    "\n",
    "    return transformed_conv, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms+\" \"+starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" +\n",
    "                  alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets +\n",
    "                  \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "\n",
    "    text = re.sub(\"([0-9])\" + \"[.]\" + \"([0-9])\", \"\\\\1<prd>\\\\2\", text)\n",
    "\n",
    "    if \"...\" in text:\n",
    "        text = text.replace(\"...\", \"<prd><prd><prd>\")\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def transformat(name):\n",
    "    with open(name + '.json', encoding = 'utf8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    convs = read_conversation(data)\n",
    "    processed_convs = []\n",
    "    for i in range(0, len(convs)):\n",
    "        temp = []\n",
    "        conv = convs[i]\n",
    "        name_prev = None\n",
    "        same_sentence = ''\n",
    "        for j in range(0, len(conv)):\n",
    "            name, sentence = extract_names(conv[j])\n",
    "\n",
    "            if name != name_prev:\n",
    "                if name_prev is not None:\n",
    "                    temp.append([name_prev, same_sentence])\n",
    "                name_prev = name\n",
    "                same_sentence = sentence\n",
    "            elif name == name_prev:\n",
    "                same_sentence = same_sentence + ' . ' + sentence\n",
    "\n",
    "        temp.append([name_prev, same_sentence])\n",
    "\n",
    "        processed_convs.append(temp)\n",
    "        \n",
    "    transformed_conv = []\n",
    "    for j in tqdm(range(0, len(processed_convs))):\n",
    "        transformed_conv.append(transform_perspective(processed_convs[j])[1])\n",
    "    \n",
    "    return transformed_conv\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [06:38<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_conv = transformat('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transformed_conv.pkl', 'wb') as f:\n",
    "    pickle.dump(transformed_conv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_triple(triples):\n",
    "    temp_set = []\n",
    "    for i in range(0, len(triples)):\n",
    "        cur = triples[i]\n",
    "        cur_subject = cur['subject']\n",
    "        cur_relation = cur['relation']\n",
    "        cur_object = cur['object']\n",
    "        \n",
    "        if len(temp_set) == 0:\n",
    "            temp_set.append([cur_subject, cur_relation, cur_object])\n",
    "        else:\n",
    "            flag = 0\n",
    "            #print(temp_set)\n",
    "            for j in range(0, len(temp_set)):\n",
    "                \n",
    "                if temp_set[j][0] == cur_subject and temp_set[j][1] == cur_relation:\n",
    "                    \n",
    "                    if len(cur_object) > len(temp_set[j][2]):\n",
    "                        temp_set[j][2] = cur_object\n",
    "                    flag = 1\n",
    "                    \n",
    "                elif temp_set[j][0] == cur_subject and temp_set[j][2] == cur_object:\n",
    "                    if len(cur_relation) > len(temp_set[j][1]):\n",
    "                        temp_set[j][1] = cur_relation\n",
    "                    flag = 1\n",
    "                    \n",
    "                elif temp_set[j][2] == cur_object and temp_set[j][1] == cur_relation:\n",
    "                    if len(cur_subject) > len(temp_set[j][0]):\n",
    "                        temp_set[j][0] = cur_subject\n",
    "                    flag = 1\n",
    "                    \n",
    "            \n",
    "            if flag == 0:\n",
    "                temp_set.append([cur_subject, cur_relation, cur_object])\n",
    "                    \n",
    "                        \n",
    "    return temp_set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/819 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "annotate() got an unexpected keyword argument 'be_quiet'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences:\n\u001B[1;32m     10\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m sent:\n\u001B[1;32m     11\u001B[0m                     \u001B[38;5;66;03m#print(sent)\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m                     triple \u001B[38;5;241m=\u001B[39m  \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mannotate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbe_quiet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mproperties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtimeout\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m5000000\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mannotators\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtokenize,ssplit,pos,depparse,parse\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutputFormat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m     16\u001B[0m \u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m                     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(triple) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     18\u001B[0m                         triples\u001B[38;5;241m.\u001B[39mextend(compress_triple(triple))\n",
      "\u001B[0;31mTypeError\u001B[0m: annotate() got an unexpected keyword argument 'be_quiet'"
     ]
    }
   ],
   "source": [
    "from openie import StanfordOpenIE\n",
    "with StanfordOpenIE(be_quiet=False) as client:\n",
    "    tuple_set = []\n",
    "    for i in tqdm(range(0, len(transformed_conv))):\n",
    "        triples = []\n",
    "        temp_set = []\n",
    "        for uttr in transformed_conv[i]:\n",
    "            sentences = sentence_tokenize(uttr)\n",
    "            for sent in sentences:\n",
    "                if \"?\" not in sent:\n",
    "                    #print(sent)\n",
    "                    triple =  client.annotate(sent,properties={\n",
    "    'timeout': '5000000',\n",
    "    'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "    'outputFormat': 'json'\n",
    "})\n",
    "                    if len(triple) > 0:\n",
    "                        triples.extend(compress_triple(triple))\n",
    "                        \n",
    "        tuple_set.append(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_raw_triples.pkl', 'wb') as f:\n",
    "    pickle.dump(tuple_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_input = []\n",
    "action_adj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_num = []\n",
    "\n",
    "for i in range(len(tuple_set)):\n",
    "    node_set = set()\n",
    "    if len(tuple_set[i]) == 0:\n",
    "        print(i)\n",
    "    for u in tuple_set[i]:\n",
    "        node_set.add(u[0])\n",
    "        node_set.add(u[1])\n",
    "        node_set.add(u[2])     \n",
    "    node_num.append(len(node_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(node_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "action_input = []\n",
    "action_adj = []\n",
    "for i in range(0, len(tuple_set)):\n",
    "    id2node = {}\n",
    "    node2id = {}\n",
    "    adj_temp = np.zeros([max(node_num), max(node_num)])\n",
    "    index = 0\n",
    "    if len(tuple_set[i]) == 0:\n",
    "        action_input.append('<pad>')\n",
    "    else:\n",
    "        temp_text = ''\n",
    "        for u in tuple_set[i]:\n",
    "            if u[0] not in node2id:\n",
    "                node2id[u[0]] = index\n",
    "                id2node[index] = u[0]\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[0] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[0] \n",
    "                \n",
    "                index = index + 1\n",
    "            if u[1] not in node2id:\n",
    "                node2id[u[1]] = index\n",
    "                id2node[index] = u[1]\n",
    "                index = index + 1\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[1] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[1] \n",
    "                    \n",
    "            if u[2] not in node2id:\n",
    "                node2id[u[2]] = index\n",
    "                id2node[index] = u[2]\n",
    "                index = index + 1\n",
    "                \n",
    "                if len(temp_text) == 0:\n",
    "                    temp_text = u[2] \n",
    "                else:\n",
    "                    temp_text = temp_text + '. </s><s> ' + u[2] \n",
    "                \n",
    "            adj_temp[node2id[u[0]]][node2id[u[0]]] = 1\n",
    "            adj_temp[node2id[u[1]]][node2id[u[1]]] = 1\n",
    "            adj_temp[node2id[u[2]]][node2id[u[2]]] = 1\n",
    "            \n",
    "            adj_temp[node2id[u[0]]][node2id[u[1]]] = 1\n",
    "            adj_temp[node2id[u[1]]][node2id[u[0]]] = 1\n",
    "            \n",
    "            adj_temp[node2id[u[1]]][node2id[u[2]]] = 1\n",
    "            adj_temp[node2id[u[2]]][node2id[u[1]]] = 1\n",
    "            \n",
    "            \n",
    "    \n",
    "        action_input.append(temp_text)\n",
    "    action_adj.append(adj_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_actions.pkl', 'wb') as f:\n",
    "    pickle.dump(action_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_action_adj.pkl', 'wb') as f:\n",
    "    pickle.dump(action_adj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly you could follow the process to create action graph files for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_actions.pkl', 'wb') as f:\n",
    "    pickle.dump(action_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_action_adj.pkl', 'wb') as f:\n",
    "    pickle.dump(action_adj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}